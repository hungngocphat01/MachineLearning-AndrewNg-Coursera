{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Import libraries"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import numpy as np \n",
                "import scipy.io as sio \n",
                "import scipy.optimize as opt\n",
                "import matplotlib.pyplot as plt \n",
                "import pandas as pd\n",
                "import time\n",
                "from IPython.display import clear_output"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Load data "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "training_data = sio.loadmat(\"ex4data1.mat\")\n",
                "X: np.array = np.array(training_data['X'], dtype=np.float128)\n",
                "y: np.array = np.array(training_data['y'], dtype=np.float128)\n",
                "# Map label 10 from matlab back to 0 (python has index 0)\n",
                "y[y == 10] = 0\n",
                "\n",
                "# Shuffle data\n",
                "dataset = np.hstack((X, y))\n",
                "np.random.shuffle(dataset)\n",
                "# Split into training set, cross validation set and test set \n",
                "train_set = dataset[None:4000, :]\n",
                "val_set = dataset[4000:4500, :]\n",
                "test_set = dataset[4500:None, :]\n",
                "\n",
                "X_train = train_set[:, None:-1]\n",
                "y_train = train_set[:, -1:None]\n",
                "\n",
                "X_val = val_set[:, None:-1]\n",
                "y_val = val_set[:, -1:None]\n",
                "\n",
                "X_test = test_set[:, None:-1]\n",
                "y_test = test_set[:, -1:None]\n",
                "\n",
                "print(\"Data loaded\")\n",
                "print(\"Size of X, y:\", X.shape, y.shape)\n",
                "print(\"Shape of X, y (train):\", X_train.shape, y_train.shape)\n",
                "print(\"Shape of X, y (vald):\", X_val.shape, y_val.shape)\n",
                "print(\"Shape of X, y (test):\", X_test.shape, y_test.shape)\n",
                "\n",
                "test_theta_data = sio.loadmat(\"ex4weights.mat\")\n",
                "TestTheta1: np.array = np.array(test_theta_data['Theta1'], dtype=np.double)\n",
                "TestTheta2: np.array = np.array(test_theta_data['Theta2'], dtype=np.double)\n",
                "\n",
                "print(\"Size of TestTheta1:\", TestTheta1.shape)\n",
                "print(\"Size of TestTheta2:\", TestTheta2.shape)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Display 100 random images"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Create 100 random indices\n",
                "# randIdx = np.random.randint(0, X.shape[0], 100).reshape(10, 10)\n",
                "# fig, ax = plt.subplots(10, 10)\n",
                "\n",
                "# for i in range(randIdx.shape[0]):\n",
                "#     for j in range(randIdx.shape[1]):\n",
                "#         example = X[randIdx[i, j]]\n",
                "#         example = example.reshape((20, 20)).T\n",
                "#         ax[i, j].imshow(example, vmin=-1, vmax=1, cmap='gray')\n",
                "#         ax[i, j].set_xticks([])\n",
                "#         ax[i, j].set_yticks([])\n",
                "# plt.show()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Define utility functions"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def sigmoid(z):\n",
                "    return 1/(1 + np.exp(-z))\n",
                "def gsigmoid(z):\n",
                "    s = sigmoid(z)\n",
                "    return np.multiply(s, 1 - s)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Define hypothesis function (feed-forward algorithm)\n",
                "\n",
                "- Init `a1=x`\n",
                "- Add bias term to `a1`\n",
                "- Calculate `z2 = Theta1 * a1`\n",
                "- Calculate `a2 = sigmoid(z2)`\n",
                "- Calculate `z3 = Theta2 * a2`\n",
                "- Calculate `h = a3 = sigmoid(z3)`"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def hypothesis(x, Theta1, Theta2):\n",
                "    a1 = np.vstack((1, x.reshape(-1, 1)))\n",
                "    z2 = Theta1 @ a1\n",
                "    a2 = np.vstack((1, sigmoid(z2).reshape(-1, 1)))\n",
                "    z3 = Theta2 @ a2 \n",
                "    h = sigmoid(z3)\n",
                "    return (\n",
                "        a1.reshape(-1, 1), \n",
                "        a2.reshape(-1, 1), \n",
                "        h.reshape(-1, 1), \n",
                "        z2.reshape(-1, 1), \n",
                "        z3.reshape(-1, 1)\n",
                "    )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Define cost function and its gradient"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def costFunction(X, y, lambd, nn_params, hidden_layer_size, output_layer_size):\n",
                "    m = X.shape[0]\n",
                "    input_layer_size = X.shape[1]\n",
                "    # nn_params: unrolled Theta1, Theta2 \n",
                "    # Extract Theta1, Theta2 from nn_params \n",
                "    Theta_vec = nn_params.reshape(-1, 1)\n",
                "    breakpnt = (input_layer_size + 1) * hidden_layer_size\n",
                "    Theta1 = Theta_vec[None:breakpnt, :].reshape((hidden_layer_size, input_layer_size + 1))\n",
                "    Theta2 = Theta_vec[breakpnt:None, :].reshape((output_layer_size, hidden_layer_size + 1))\n",
                "    \n",
                "    J = 0 \n",
                "    Theta1_grad = np.zeros(shape=Theta1.shape)\n",
                "    Theta2_grad = np.zeros(shape=Theta2.shape)\n",
                "    Delta1 = 0 \n",
                "    Delta2 = 0\n",
                "    for i in range(X.shape[0]):\n",
                "        x = X[i, :].reshape((-1, 1))\n",
                "        # Feed-forward\n",
                "        a1, a2, h, z2, z3 = hypothesis(x, Theta1, Theta2)\n",
                "        # print(np.sum(a1), np.sum(z2), np.sum(a2), np.sum(z3), np.sum(h))\n",
                "        # break\n",
                "        # Compute J \n",
                "        y_hop = np.zeros(output_layer_size).reshape(-1, 1)\n",
                "        y_hop[int(y[i])] = 1\n",
                "        j = -y_hop.T @ np.log(h) - (1 - y_hop).T @ np.log(1 - h)\n",
                "        J += float(j)\n",
                "\n",
                "        # Back propagation \n",
                "        delta3 = h - y_hop \n",
                "        delta2 = np.multiply(Theta2.T @ delta3, gsigmoid(np.vstack((0, z2))))\n",
                "        delta2 = delta2[1:None]\n",
                "        Delta1 += delta2 @ a1.T \n",
                "        Delta2 += delta3 @ a2.T \n",
                "    # Average \n",
                "    J *= 1/m \n",
                "    # Add regularized term for cost function \n",
                "    theta1_reg = Theta1\n",
                "    theta1_reg[:,0] = 0 \n",
                "    theta2_reg = Theta2\n",
                "    theta2_reg[:,0] = 0 \n",
                "    J += lambd/(2*m) * (np.sum(np.power(theta1_reg, 2)) + np.sum(np.power(theta2_reg, 2)))\n",
                "    # Regularized term for theta_gradient \n",
                "    Theta1_grad = (1/m * Delta1) + (lambd/m) * theta1_reg\n",
                "    Theta2_grad = (1/m * Delta2) + (lambd/m) * theta2_reg\n",
                "\n",
                "    # Unroll gradient \n",
                "    grad = np.vstack((Theta1_grad.reshape(-1, 1), Theta2_grad.reshape(-1, 1))).reshape(-1, 1)\n",
                "\n",
                "    return (float(J), grad)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Define functions to calculate and check numerical gradients"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def numericalGradient(J, theta):\n",
                "    numgrad = np.zeros(shape=theta.shape)\n",
                "    for i in range(len(theta)):\n",
                "        e = np.power(10.0, -4, dtype=np.double)\n",
                "        epsilon_vec = np.zeros(shape=theta.shape)\n",
                "        epsilon_vec[i] = e \n",
                "\n",
                "        left_j, _ = J(theta - epsilon_vec)\n",
                "        right_j, _ = J(theta + epsilon_vec)\n",
                "        numgrad[i] = (right_j - left_j)/(2*e)\n",
                "    return numgrad\n",
                "\n",
                "def debugInitializeWeights(out_conn, in_conn):\n",
                "    W = np.zeros((out_conn, 1 + in_conn))\n",
                "    W = (np.sin(np.array(range(1, W.size+1)))/10).reshape(W.shape)\n",
                "    return W\n",
                "\n",
                "def initializeRandomWeights(L_in, L_out):\n",
                "    W = np.zeros((L_out, 1 + L_in))\n",
                "    e = 0.12 \n",
                "    W = np.random.rand(L_out, 1 + L_in) * 2 * e - e\n",
                "    return W\n",
                "\n",
                "\n",
                "def checkNumGradient(lamd):\n",
                "    input_layer_size = 3\n",
                "    hidden_layer_size = 5 \n",
                "    num_labels = 3 \n",
                "    m = 5\n",
                "\n",
                "    Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)\n",
                "    Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)\n",
                "    X = debugInitializeWeights(m, input_layer_size - 1)\n",
                "    y = (np.array(range(1, m+1)) % num_labels).reshape(-1, 1) \n",
                "\n",
                "    cost_func_ptr = lambda theta: costFunction(X, y, lamd, theta, hidden_layer_size, num_labels)\n",
                "    nn_params = np.concatenate((Theta1.reshape(-1, 1), Theta2.reshape(-1, 1)), axis=0)\n",
                "    # Compute numerical gradient\n",
                "    _, grad = cost_func_ptr(nn_params)\n",
                "    \n",
                "    num_grad = numericalGradient(cost_func_ptr, nn_params)\n",
                "\n",
                "    print(\"Visual check\")\n",
                "    print(np.hstack((num_grad.reshape(-1, 1), grad.reshape(-1, 1))))\n",
                "\n",
                "    diff = np.linalg.norm(num_grad - grad)/np.linalg.norm(num_grad + grad)\n",
                "    print(\"Errors:\", diff)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Check backpropagation implementation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# checkNumGradient(0)\n",
                "# checkNumGradient(10)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Define gradient descent function"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "scinotation = lambda f: '{:.2e}'.format(f)\n",
                "\n",
                "def NesterovGradientDescent(cost_func, gamma, learning_rate, init_theta, max_iters):\n",
                "    # Prepare data for gradient descent\n",
                "    theta = init_theta.reshape(-1, 1)\n",
                "    v = np.zeros_like(theta)\n",
                "    change_rate = 0\n",
                "\n",
                "    i = 0\n",
                "    for i in range(max_iters):\n",
                "        J, grad = cost_func(theta - gamma * v)\n",
                "\n",
                "        grad_length = np.linalg.norm(grad)/len(grad)\n",
                "        if grad_length < 5e-6:\n",
                "            print(\"\\nStopped early\")\n",
                "            break\n",
                "\n",
                "        v = v * gamma + learning_rate * grad \n",
                "        theta = theta - v \n",
                "        \n",
                "        print(\"\\r\", \n",
                "            \"Iteration:\", i, \n",
                "            \"; Gradient length:\", scinotation(grad_length),\n",
                "            \"; Cost value:\", scinotation(J), \n",
                "            \"; Gradient:\", scinotation(np.mean(grad).flatten()[0]), end=\"\"\n",
                "        )\n",
                "    \n",
                "    return theta, i"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Benchmarking functions"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def calculateAccuracy(X, y, Theta1, Theta2):\n",
                "    correct_counter = 0\n",
                "    for i in range(X.shape[0]): \n",
                "        x = X[i, :].reshape(-1, 1)\n",
                "        _, _, h, _, _ = hypothesis(x, Theta1, Theta2)\n",
                "        label = np.argmax(h)\n",
                "        if (label == y[i]):\n",
                "            correct_counter += 1\n",
                "    return correct_counter/X.shape[0]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Train neural network"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "input_layer_size = X.shape[1]\n",
                "hidden_layer_size = 25\n",
                "output_layer_size = 10\n",
                "\n",
                "lambds = [1, 10, 20, 40, 80, 100]\n",
                "Thetas = []\n",
                "cross_errors = []\n",
                "iterations = []\n",
                "accuracies = []\n",
                "for lambd in lambds:\n",
                "    print(\"Initializing neural network\")\n",
                "    init_Theta1 = initializeRandomWeights(input_layer_size, hidden_layer_size).reshape(-1, 1)\n",
                "    init_Theta2 = initializeRandomWeights(hidden_layer_size, output_layer_size).reshape(-1, 1)\n",
                "    init_Theta = np.vstack((init_Theta1, init_Theta2)).reshape(-1, 1)\n",
                "\n",
                "    cost_func_ptr = lambda theta: costFunction(X_train, y_train, lambd, theta, hidden_layer_size, output_layer_size)\n",
                "    \n",
                "    print(\"Begin training with lambda=%d\" % lambd)\n",
                "    trained_Theta, iters = NesterovGradientDescent(cost_func_ptr, 0.9, 0.4, init_Theta, 150) \n",
                "    # Extract back theta1, theta2\n",
                "    breakpnt = (input_layer_size + 1) * hidden_layer_size\n",
                "    Theta1 = trained_Theta[None:breakpnt, :].reshape((hidden_layer_size, input_layer_size + 1))\n",
                "    Theta2 = trained_Theta[breakpnt:None, :].reshape((output_layer_size, hidden_layer_size + 1))\n",
                "    Thetas.append((Theta1, Theta2))\n",
                "\n",
                "    cross_error, _ =  costFunction(X_val, y_val, 0, trained_Theta, hidden_layer_size, output_layer_size)\n",
                "    cross_accuracy = calculateAccuracy(X_val, y_val, Theta1, Theta2)\n",
                "    print(\"Cross validation error:%f\" % cross_error)\n",
                "    print(\"Cross validation accuracy:%f\\n\" % cross_accuracy)\n",
                "    print(\"Iterations: %d\" % iters)\n",
                "    cross_errors.append(cross_error)\n",
                "    iterations.append(iters)\n",
                "    accuracies.append(cross_accuracy)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Show learning results"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "result = pd.DataFrame()\n",
                "result['lambda'] = lambds\n",
                "result['iters'] = iterations\n",
                "result['validation'] = cross_errors\n",
                "result['accuracy'] = accuracies\n",
                "print(result)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Demo"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "while True:\n",
                "    try:\n",
                "        # Choose a random number\n",
                "        randIdx = np.random.randint(0, X.shape[0] - 1)\n",
                "        x = X[randIdx]\n",
                "        # Predict\n",
                "        _, _, h, _, _ = hypothesis(x, Theta1, Theta2)\n",
                "        # Show result\n",
                "        label = np.argmax(h)\n",
                "        img = x.reshape((20, 20)).T\n",
                "        plt.imshow(img)\n",
                "        plt.title(f\"Label: {label}\", fontdict={\"fontsize\": 30})\n",
                "        plt.show()\n",
                "        time.sleep(1)\n",
                "        clear_output(wait=True)\n",
                "    except KeyboardInterrupt:\n",
                "        print(\"Stopped\")\n",
                "        break\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit"
        },
        "interpreter": {
            "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}